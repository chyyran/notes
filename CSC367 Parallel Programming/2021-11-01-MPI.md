# 2021-11-01 MPI

* Performance properties of network
* latency   
  * diameter
    * maximum over all pairs of nodes of thee shortest path between a given pair of nodes
    * in any 2 nodes, find the shortest path that would connect it.
    * go through all pairs of nodes, then pick the maximum
    * find the furthest 2 nodes, and pick the shortest path
  * latency
    * within the same system, not related to distance
    * delay between send and and receive times
    * latency tends to vary widely across architectures
    * vendors often report hardware latencies (wire time)
    * application programmers care about software latency (user program to program)
      * software latency is ping/pong roundtrip
      * many people use 1/2 roundtrip time to approximate 1-way latency
    * latency is **key** for programs with many small messages
    * latency has not improved significantly unlike Moore's law
      * communication is the problem, latency bottleneck
      * lowest latency was t32 (shmem) in 1997
* bandwidth
  * bisection bandwidth is the smallest cut that divides the network into 2 halves
  * bandwidth across the narrowest part of the network
  * cut of 1-d network is the same because we only make 1 cut
    * bisection bw = link bw
  * cut of 2d network makes n cuts 
    * bisection bw = sqrt(p) * link bw
    * p is number of processors so number of edges is going to be sqrt(p)
* linear or ring topologies
  * linear array
    * diameter = n-1
    * bisection bandwidth = 1lbw 
  * torus/ring
    * diameter = n/2
    * bisection bandwidth = 2lbw 
* 2D topologies
  * two dimensional mesh
    * diameter = 2 * (sqrt (n) - 1)
      * we already said that number of links in an edge is sqrt(n)
    * bisection bandwidth = sqrt(n) lbw
  * two dimentional torus
    * diameter = sqrt(n)
    * bisection bandwidth = 2sqrt(n) 
  * natural from algorithms that work with 2D or 3D arrays
* this generalizes to higher dimension
  * Cray XT uses 3D Torus
* tree networks
  * bisection bandwidth = 1
  * easy layout as planar graph
  * many tree algorithms benefit from tree topologies
  * **fat trees** avoid bisection bandwidth problem
    * more or wwider links near the top
  * allows neihgbouring machines to communicate faster
* message passing model
  * single program multiple data
  * all processes execute same code, communicate via messages
  * technically does support different programs on each host/process
* why message passing
  * build a parallel multi-computer from lots of nodes
  * nodes are connected with a network
  * partition data across the nodes, comuptation in parallel
  * if local data needed on remote node send it over interconnect (this is expesive)
  * computation done collectiviely
  * can always add mroe nodes, bottleneck is not the number of cores or memory on a single node
    * scale out instead of up: can increase data size while increasing processors
  * downside is harder to program
    * every communication has to be hand-coded by the developer
---

## Test format
* MC, short answer, some computational complexity of MxM computation