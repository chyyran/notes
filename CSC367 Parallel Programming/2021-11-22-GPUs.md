# 2021-11-22 GPUs

* Connected to the CPU via PCIe bus
* GPU has its own memory (usually much smaller than CPU memory)
* stencil computations have same kind of pattern as graphics data
* [CPU Memory] <- 10-25GB/s -> CPU <- PCI bus 8-16GB/s -> GPU <- 100-300  GB/s->
* Bottlenecked by the PCIe bus
* CPUs are latency optimized and GPUs are throughput optimized but hides latency pretty well with a cache
  * CPUs optimized for fast serial processing
  * GPUs optimized for scalable parallel processing
    * number of threads spawned on a GPU is significantly more than the number of cores
    * hides latency by scheduling threads once one is done
* CPU vs GPU architecture
  * CPU optimized for low-latency access to cached data sets
    * Control logic, few ALUs, and large cache + DRAM layer
  * GPU is throughput optimized
    * ideal for parallel computation
    * each streaming multiprocessors have N ALU cores cores access to same cache and small control logic
    * more transistors dedicated to computation
    * DRAM layer above everything 
    * Can not execute complicated applications
    * large amount of SIMD multiprocessors
* conceptual approach
  * add arrays A and B into C
  * CPU (seq): allocate memory, for loop to add pairwise
  * CPU (thread): create N threads = cores, partition arrays into ranges, and pairwise per thread
    * 8 cores => 8Max = ~8X
    * can't scale better because of thread limit
    * limit cores, memory contention etc
  * GPU approach
    * allocate memory for arrays on GPU
    * transfer CPU -> GPU
    * launch kernel (function by each thread)
      * massive amount of threads (eg. 32k, 64k, etc)
      * each thread isntructed to handle few elements
    * wait for threads to finish
    * transfer results back to CPU
  * GPU hides memory latency better than CPU
    * GPU switches between threads to hide latency
    * threads do lightweight jobs with tons of registers and switching is basically free
    * threads have basically 0 context compared to CPU
* GPU performance scaling is exponential up to 2014 at least.
  * GFLOP performance on MxM
* https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf
* Compute capability
  * tells us what features supported by GPU
  * CUDA SDKs have different versions
  * can compile code for different capabiities but not for a higher capability than GPU
  * higher capabilities have more features
  * compute capability tries not to change
    * warp size = vector reg size in SIMD
    * thread block is number of threads put in a block
      * threads inside threadblock can talk to each other cheaply
* CUDA architecture
  * SMs with L2 cache between
  * 64 bit memory controllers handle throughput between L2 and global GDDR ram
  * each SM has thousans of registers with
    * shared memory
    * constant cache
    * texture cache
    * L1 cache
    * warp scheduler that schedules work on the SM
* CUDA debugging
  * Nsight is visual
  * nvprof or nvvp ise useful to analyze programs
    * nvprof profiling result is useful
    * nvprof API can be turned off to reduce profiling overhead (--profile-api-trace none)
    * printing on device involves data transfer (even cuPrintf)
    * cuda-gdb works with -g (host code) and -G (device code)
* typical cuda program
  * allocate memory on host (CPU)
  * allocate memory on device (GPU)
  * transfer data to device memory
  * launch kernels
  * wait to finish
  * transfer data back to host
  * identifier tells compiler what to executre
    * `__device__` executed on GPU, called by GPU
    * `__global__` executed on GPU, called by CPU
    * `__host__`/default = CPU code
  * kernel blocks launched with `<<<>>>` blocks
* threads can be organized in 1D, 2D or 3D layouts within a block for easier indexing
  * schedueld in warps (batches of 32 threads)
  * when you create X number of thread blocks, from then on any synchronization between threads of different blocks is extremely expensive
    * has to be atomic through global memory
    * 0 guarantee of which thread block executes where and when
