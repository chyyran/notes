# 2021-09-29 BLAS

## BLAS (Basic Linear Subroutines)
* BLAS1 (1970s) 15 diff ops
  * vector ops: dot product, saxpy, etc
  * \(m = 2n\), \(f=2n\), \(q \approx 1\) or less.
* BLAS2 (1980s): 25 diff ops
  * matrix-vector ops
  * \(m = n^2\), \(f = 2n^2\), so \(q \approx 2\)
  * slightly better than BLAS1
* BLAS3 (late 1980s) 9 diff ops
  * matrix-matrix ops
  * \(m \leq 3n^2\) \(f = 2n^2\), \(q \approx n\) potentially
  * BLAS3 is potentially much faster than BLAS2
* Libraries are LAPACK, ScaLAPACK
  * written in fortran
* BLAS2 and BLAS3  have very different computational intensity
  * fastest, most optimized version is BLAS3 (DGEMM)
  * the optimizations hide overhead for big data


## Essential components of parallel architectures
* Bunch of memory is connected by a network, whether internal or external
* The way of connecting gives us 4 classes of parallel machines
  * Shared memory machines (pthreads, OpenMP)
    * DRAM is shared
    * network on chip
    * cores that share memory
  * Distributed Memory machines (MPI)
    * Independent machine with own DRAM
    * external, low latency network
    * eg. SciNet
  * SIMD and Vector (OpenCL, CUDA)
    * entire architecture is an entire vector unit
    * GPU
  * Hybrid: A mix of the above

* Parallel Algorithm Design
  * Identify tasks in your program that are concurrent
  * Map concurrent tasks onto multiple threads or processors
  * Partition I/O or intermediate data and assign to processes
  * Handle concurrent access to shared data by multiple processes
  * Add synchronization where necessary
  * Keep in mind underlying architecture
  * Profile to determine bottlenecks
  * Target optimizations based on profiling
  * Write benchmarks to test program
* Decomposition
  * divide computation into tasks that can be executed in parallel
  * With multiple machines, program-level parallelism is good
  * Instruction-level parallelism would require a bunch of communication on distributed memory machine
    * However, on shared memory machine where communication is free, this will allow cache reuse
* Task
  * Unit of computation that can be extracted from the main program and assigned to a concurrent process
  * Method of extraction and mapping affects performance
  * Can be on the scale of single instruction to whole programs
  * Can have dependencies
    * might need data produced by other tasks and wait till input is ready
    * tasks without dependencies are independent tasks
    * dependencies create an ordering of task execution (task dependency graph)
      * DAG: tasks as nodes, dependencies as edges
      * start nodes have no incoming edges, finish nodes no outgoing edges
  * Fine vs Coarse grained
    * Fine grained: each task processes single element
      * frequent communication is necessary
      * suitable for shared memory environments
    * Coarse grained: each task processes half the elements of the array
      * lots of computation performed before computation necessary
      * good match for message passing environments
    * communication between tasks may or may not be necessary
    * Ideally there is no need to communicate
    * **Parallelism granularity** is how much processing is performed before communication is necessary between processes
* Degree of concurrency
  * Max degree: max number of tasks that can be executed simultaenously
  * Average degree of concurrency = total amount of work / critical path length
  * Assign weights to nodes to represent amount of work
  * Critical path: longest path between any pair of start and finish nodes
  * CPL: sum of node weights along critical path
  * **Shorter critical path means higher degree of concurrency** 