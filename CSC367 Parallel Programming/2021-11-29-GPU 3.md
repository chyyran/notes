# 2021-11-29 GPU 3

* If max number of thread per SM is 2048, max number of blocks per SM is 32 and max regs per SM is 64K.
  * With 256 t/b, SM can only run 8 thread blocks because 2048/256 = 8
  * 256 t/b and each thread uses 48 regs, each SM runs 5 thread blocks
    * block needs 256 * 48 = ~14K registers, 64K/14K = ~ 5
    * commpiler will reduce number of active threads to reduce spilling regs to mem 
* Memory Types
  * global memory
    * does not live in SM, space outside of SM
    * most data lives here
    * host communication (where dat agets xferred to GPU)
    * shared by all threads
    * large size (few GB) slower than shared memory
    * l1 cache helps hide latency for global and local mem access
    * **good bandwidth via coalescing**
      * warp should reference sequential mem locations for best performance
        * access get coalesced into a single access
  * local memory
    * take part of a global memory that is coalesced
    * private per thread global mem
    * auto variables, reg spill
    * same speed as global memory but coalesced
  * shared memory
    * lower latency than global memory
    * on an SM and it's fast
    * software programmable L1 cache indicated by `__shared__`
    * organized in 32 banks
      * successize 32-bit words are assigned to successive banks
      * mem load/store of N addresses spanning N distinct memory banks can be **serviced simultatenously**
        * N times bandwidth of single bank
      * bandwidth is 32bits per bank per cycle
    * bank conflicts: failure to distribute thread access across memory bank
      * if threads in a warp access distinct  banks, access can happen in one cycle
      * if threads in a warp access diff data in the same banks, access becomes serialized
      * if threads in a warp access same word in one bank, that's fine.
    * capacity = 64k/32bank/wordsize=32bit
    * shared by all threads in a block, and provides mechanism for threads to cooperate
      * may need to use `__synthreads()` for block level barriers
    * facilitates global mem coalescing where it would not otherwise possible
      * does not have sequential access restrictions of global memory 
    * only need to to avoid bank conflicts
      * serialized accesses may be worse than global memory
  * constant memory
    * used for data that does not change during kernel execution
    * small amount, typically 64K
    * single read can be broadcast to a halfwarp
      * saves up to 15 reads and reduce required memory bandwidth by 94%
    * cached
      * consecutive reads from same address does not inclur additional memoryt raffic
      * halfwarp threads read from same location
    * use `__constant__` modifier to indicate data is stored there
  * texture memory
    * normally not that easy to use
    * bindless textures `cudaTextureObject_t` in CUDA 5.0
    * special type of read only memory
      * for data spatially close together
      * thread reads from memory location nearby where other threads are also reading
    * when to use?
      * rarely update data but often read
      * read access pattern presents some spatial locality
    * when not
      * when data is read rarely after being written
* reduction
  * consider an array fitting in global memory that we want to reduce
  * split it, have each block reduce
  * very simple solution is to have an atomic to collect the reduced results
  * 