# 2021-10-06 Mapping Techniques

* Every step in the dependency tree is a sequential operation
  * How many sequential steps depends on how many levels, so if we have n leaves our tree is log(n)
* Why do we care about mapping the task?
  * what if we randomly map tasks to processors?
  * an efficient task mapping is critical top minimize parallel overhead
* the goal is to complete all tasks in shortest possible time
  * we want to minimize the overhead of task execution
    * **load balancing**: minimize time spent idle by the process
    * **minimize time in interactions** among processes
  * these goals are conflicting
    * to optimize interactions, we want to put interacting tasks on the same processor
      * this can lead to load imbalance and idling, for example all tasks to same processor
    * to optimize load balancing, we want to break down tasks into fine grained pieces
      * this can lead to a lot more interaction overhead
  * we need to carefully balance these two goals.
    * if one core is idle, it can grab a new job, for example
* A balanced load may not necessarily mean no idling!
  * Work may be carried out in stages, but assigned workload may not balanced for every stage
  * If tasks have some data dependency, even if each process has equal amount of tasks, they may have to idle to wait for data
* static mapping
  * assign tasks to processes before execution starts
  * allows for static load balancing
  * mapping quality depends on knowledge of task sizes, size of data associated with tasks, characteristics of task interactions, and parallel programming paradigm
    * **if task sizes not known, this can potentially lead to severe load imbalances**
  * usually done with data parallel problems
  * tasks are tied to chunks of data generated by partitioning approach
  * closely tied to mapping data to processes
  * often useful for deep-data computing
    * large amounts of data lives on machine A, so we tell the task to go to machine A and do the computation
  * simple example
    * Matrix/Vector mult
    * 4 tasks, each task computes 2 elements of C
    * give each task 2 rows
    * **we know the size of the task a priori**
* dynamic mapping
  * assign tasks to processes during execution
  * allows for dynamic load balancing
  * **effective for unknown task sizes**
  * if more data than computation, there is large overhead for data movement, then static mapping may be preferable
  * depends on the parallel paradigm and interaction type
    * shared address space vs distributed memory
    * ro/rw interaction
  * a pool of tasks is put in a work queue
    * process managing pool of ready tasks is the 'master process'
    * other processes performinign the tasks are worker or slave processes
  * tasks may get added to the pool, concurrently with the workers taking tasks out
* need to reduce interactions
  * maximize data locality
    * goals
      * minimize volume of interaction overheads
        * minimize volume of shared data and maximize cache reuse
        * use a suitable decomposition and mapping
        * use local data to store intermediate results
      * minimize the frequency of interactions
        * restructure the algorithm to access and use large chunks of shared data
        * shared address space: spatial locality in a cache line
  * minimizing contention and hot spots
    * often used for distributed systems
    * accessing shared data concurrently can generate contention
      * concurrent accesses to same memory block, flooding process with message
    * solution is to
      * restructure the program to reorder access in a non-contentious manner
      * decentralize shared data to eliminate single point of contention
        * gradually work towards a unit of work to avoid everyone writing to IO at the same time
        * rather than all tasks writing to a single output task
  * overlapping computations with interactions
  * replicating data or computations
* Overlap computations with interactions
  * very common that one process has to idle to wait for another process to finish or an interaction to finish
  * what if we can initiate the action before its needed?
    * ensure that the interaction is ready when needed
  * grab more tasks in advance, before current task is completed
  * may be supported in software (compiler, OS) or hardware (prefetching, branch prediction)
  * harder to implement with shared memory models (pthreads,  OpenMP), applies more to distributed and GPU architectures
  * branch prediction
    * if condition has a very complex condition to resolve
    * we're going to predict that the branch is true based on profiling data
    * grab all the true inistruction and start executing
    * by the time the branch condition is resolved, if it resolves correctly then we have already done the computation
    * if not, we just discard computations
  * data prefetch/instruction prefetch
* replicate data or computations
  * reduce contention for shared data
  * this will reduce interaction overhead
  * beneficial if the shared data is accessed in a read only fashion
    * shared-address space paradigm will cache local copies of the data
    * message-passing paradigm will replicate data to eliminate data transfer overhead
  * do extra computation amortized by reducing huge communication time
  * disadvantages
    * increase overall memory usage by keeping replicas
    * if shared data is RW, we have to keep copies coherent
      * coherency overhead may dwarf benefits of local access via replication