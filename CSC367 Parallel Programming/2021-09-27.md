# 2021-09-27

## Block Tiled Matrix Multiple
Consider `A, B, C` to be \(N \times N\) matrices of \(b \times b\) subblocks where \(b = n / N\) called thee block-size.
Assume 3 blocks stick into cache at one time.
```rust
for i in 1..N {
  for j in 1..N {
    // 1. read block C(i, j) into fastmem
    for k in 1..N {
      // 2. read block A[i, k] into fastmem
      // 3. read block B[k, j] into fastmem
      C[i, j] = C[i, j] + A[i, k] * B[k, j] { .. }; // do matrix multiply on blocks, 3 nested loops inside
    }
    // 4. write block C(i, j) back to slow mem
  }
}
```

* \(m = N * N ((n/N)^2 + N * ((n/N)^2 + (n/N)^2) + (n/N)^2) = (2N+2)n^2\)
  * Step 1 takes \(b^2\) memory accesses within an \(N^2\) loop
  * Step 2 and 3 take \(2b^2\) within an additional \(N\) sized loop
  * Step 4 takes \(b^2\) access to write back to slowmem
  * However, steps 2 and 3 are free for each inner access withihn the hidden nested loop because they fit in cache!
  * `A` and `B` are read \(N\) times. `C` is read once, and written once
    * \(N*n^2\): read each block of `B` \(N^3\) times 
    * \(+ N*n^2\): read each block of `A` \(N^3\) times
    * \(+ 2^n^2): read each block of `C` once.
* Computational intensity \(q\) is about \(n / N = b\) for large \(n\).  
  * We can improve performance by increasing \(b\)
  * If we increase \(b\) to be the same size as the matrix then we're back on the ideal consumption
    * Why not increase \(b\)?
    * 3 blocks of \(b\) have to fit into cache.
    * To compute the upper bound of \(b\).
    * Consider \(M_{fast}\) is the size of fast memory, then we have \(q \approx b \leq \sqrt{M_{fast}/3}\)
  * We get reuse here!
* If we have mutiple caches, we need to nest tiling
* We can tile for registers (compiler) or for caches (hardware)
## BLAS (Basic Linear Subroutines)
* BLAS1 (1970s) 15 diff ops
  * vector ops: dot product, saxpy, etc
  * \(m = 2n\), \(f=2n\), \(q \approx 1\) or less.
* BLAS2 (1980s): 25 diff ops
  * matrix-vector ops
  * \(m = n^2\), \(f = 2n^2\), so \(q \approx 2\)
  * slightly better than BLAS1
* BLAS3 (late 1980s) 9 diff ops
  * matrix-matrix ops
  * \(m \leq 3n^2\) \(f = 2n^2\), \(q \approx n\) potentially
  * BLAS3 is potentially much faster than BLAS2