# 2021-11-24 GPU 2
* grids and blocks
  * scheduled in warps (batches of 32 threads)
  * warp execution order is undefined and up to hardware
  * thread inside of threadblocks are assigned on grid
  * all of the threads inside one thread block get assigned on one SM
    * same regfile, same shared memory
* a block is a logical organization of thread
  * could have arbitrary 2^n # threads
  * if you spawn off too many blocks they will get queued
  * threads inside a block can sync pretty easily
  * threads between a block execute in any order cand cannot cooperate
* a kernel gets assigned to a grid which contains the block
  * grid can be organized as 1d,2d,3d depending on indexing
* example increment all elements in N*N matrix
  * assign 1 element per thread, N=1024
  * block size = 256 thread => 1024^2/256 = 4096 blocks
    * 2D blocks = 16^2 threads/block
    * blocks in 2D grid => 64^2 blocks
  * CUDA C
    * ```cpp
      dim3 threadsPerBlock(16, 16) // 256 t
      dim3 blocks(N /threadsPerBlock.x, N/threadsPerBlock.y) //4096 blocks
      compute<<<blocks, threadsPerBlock>>> (/* kernel params */) // kernel
      ```
  * blocks and thread/block have to be specified by the programmer
  * builtin notations which a thread can use to identify its index in grids and blocks
    * `threadIdx` = thread index **within block**
    * `blockDim` = size of a block (threads in each dimension)
    * `blockIdx` = block index in the grid
    * `gridDim` = size of a grid (blocks in each dimension)
  * thread can identify assigned element as follows
    * `int i = (blockIdx.x * blockDim.x) + threadIdx.x;`
    * `int j = (blockIdx.y * blockDim.y) + threadIdx.y;`
* parallel exec
  * single instruction is issued for a warp at a time
  * threads in a warp execute instructions in lockstep (same instr for all)
    * implicit barrier for all threads in each warp
  * warps can run ahead of other warps - use `__syncthreads()` to barrier all thread warps (not all threads from all blocks!) in a block
  * on one cycle, warp and do same instruction for all threads
  * all threads in warp execute same instruction
    * there needs to be enough warps to hide memory access latency
    * when thread warps go through different paths we get **thread divergence**
      * `if { a } else { b }` will take 2 cycles
        * threads that take `a` will run first, and thread for `b` will stall
        * at the next cycle, threads that run `b` will run after
      * the trick to avoid this is to schedule branching at warp granularity
* limitations
  * max number of threads per block that can execute
  * max number of blocks per SM
    * we can't control this. this is because of resource limitations
    * single most important thing that affects resource limitations
  * max number of blocks per grid
  * these numbers depend on GPU and are hardware limitations.
    * if we exceed this kernel launch will fail
    * for large data we can't rely on one item per thread
  * sometimes less is more!
    * best resource allocation is about 66%