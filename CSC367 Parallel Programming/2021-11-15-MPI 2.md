# 2021-11-15 MPI 2

* Flavours of communication
  * Collective: all proceses in communicator or group participate
  * Point to point
    * Blocking send/recv
      * `MPI_Send`/`MPI_Recv`
      * Each message has a tag associated to distinguish it
      * can be `MPI_ANY_SOURCE`/`MPI_ANY_TAG`
      * Receives has a status object to see  source, tag, and potential error
      * `MPI_Get_count` can find length of the received message
      * recv only returns after message is received and compied into buffer
        * buffer can be safely reused after MPI_recv
      * send can be implemented two options
        * option 1: returns only after matching MPI recv is sent
        * option 2: copy message into buf and returns without waiting for MPI_recv
          * potentially faster
        * buffer can be safely reused right after MPI_send in both options
        * if order of send/recv does not match there will be a deadlock
      * circular chains of send/recv
        * consider 
          * ```
            for each program myrank of np:
                MPI_Send(..., (myrank+1)%np)
                MPI_Recv(..., (myrank-1+np) %np)
            ```
            this will deadlock. to fix this we need to make odd-numbered processors first receive while even number processors receive
            this way we do not have a circular deadlock where each consecutive processor waits for the previous one.
            ```
            for each program myrank of np:
                myrank % 2 = 0 => 
                    MPI_Recv(..., (myrank-1+np) %np)
                    MPI_Send(..., (myrank+1)%np)
                myrank % 2 = 1 => 
                    MPI_Send(..., (myrank+1)%np)
                    MPI_Recv(..., (myrank-1+np) %np)
            ```
        * Equivalent to `MPI_Sendrecv` which takes care of this, but send/recv buffers must be disjoint
    * Non-blocking send/recv
      * `MPI_Isend/MPI_Irecv`
      * We have to use `MPI_Test` or `MPI_Wait` to determine whether or not a non-blocking operation to wait until non-blocking op is completed
      * MPI_Test checks if message properly received, does not block
      * MPI_Wait actually waits for the message to fully be received
* Collective operations
  * Barrier
    * blocks all proceses until communicator hits barrier
    * barrier does not wait for pending non-blocking operations
      * if you use non-blockign s/r, you have to use MPI_Wait
  * Broadcast
    * One to all
    * One machine will send something to all machines
    * Receivers do not have to call MPI_Recv
    * blocks until all processes make a matching MPI_Bcast call
      * not required to block until op fully completes
    * more efficient than s/r
      * optimized internally by the MPI library
      * Bcast uses a tree-based hierarchy and removes contention from root
  * Reduction
    * combines elements from buffer of each process in group and stores recv buf at the target receiver
    * all processes must provide send and recv of same size a nd data type
    * built in operations:
      * MPI_SUM/MPI_MAX/MPI_MIN/MPI_PROD, etc.
      * Can have user-defined operations as well
    * Allreduce does not have a target and everyone gets the result of the reduce
  * Scatter
    * sends a different part of sendbuf to all others including itself
    * sendcount is number of elements sent to each process
    * all processors receives same amount
    * MPI_Scatterv allows different number to be sent to each of receivers
  * Gather
    * Each process  sends sendbuf data to the target process
    * recev buffer only receives number of elements per processor
    * Allgather is where we take everything from all the machines, then gather it to eevery other machine
  * Alltoall
    * sends a different portion of sendbuf (sendcount contig. items) to other processes to each other process
* MPI drawbacks
  * not fault-tolerant
  * we need checkpointing, where constantly data is checked against previously stored state
  * in cloud computing, the scheduler handles where the code is run and is automatically handled
    * does handicap on the type of code that can be written