# 2021-06-09 Perception

## Categorical perception
  * VOT is a continuum
  * different languages break up continuum differently
  * phonetic implementation of voicing contrast differs between languages
    * eg: english have partially voiceless stops and voicedless are aspirated (positive VOT)
      * spanish: voiced stops are fully voiced (low VOT)
  * languages break up VOT in categories
    * brains like to quantize continuum as categories
      * voiced vs voiceless/aspirated vs voiceless
      * we perceive something as one sound until we suddenly perceive it as another
      * *categorical perception*
      * vowels, consonants and tones are all perceived categorically
    * suddenly realize that you switch phonem boundary
      * VOT passes threshold for /d/ and becomes /t/ 
* Accents
* Parsing the speech stream
* non-auditory information
* slips of the ear
* seen for place, manner
* esh and /s/ also categorically perceptualized
  * /s/ has increased intensity at higher freqs (higher COG)
  * data has an S-curve pattern
    * **expected pattern for categorical perception**
    * if data was perceived continuously it would be straight diagonal
  * ambiguous token between /s/ and esh is perceived as /s/ about 50% of the time (avg)
* /s/ and /esh/ produced by women in general have high intensity at higher freq = higher COG (strand, johnson 96)
  * sex and gender
  * females on avg have smaller vocal tracts than men
  * acoustic diff in /s/ and /esh/ articulate differently to exaggerate gender diff in speech
    * primarily white americans
    * /s/ esh diff does not exist in all north american english speakers
    * certain african american populations do not exhibit this diff
  * perception of ambiguous token changes depends on whether the rest of the word was said by man or woman
  * ambiguous token was too low to be a female /s/ but high enough to be male /s/
  * information about speaker is taken into account by perceptor -- speaker normalization
  * using exact same stimuli with diff gender video also shifted perception of ambiguous tokens
## Tone and Categorical Perception
* not everyone is familiar with tone because not all languages have it
* speakers of tonal languages have categorical perception
* speakers of non-tonal languages do not but can develop it
* compared perception of T2 and T3 in L1/advanced L2/no knowledge of mandarin speakers
  * S-curve
  * L1 and L2 have similar discrimination score curve
  * no knowledge do not have categorical perception
    * use general pitch perception
    * english speakers are using pitch as a continuum
* tonal language speaker or other language tones
  * type and complexity of tone matter
  * speakers of thai were better at **producing** mando tones than yoruba (register tone) (yang 2018)
  * thai listeners not better than indonesian listeners in telling apart mando tones
    * thai listener discrimination was diff than indonesian listeners
    * thai listener perception was perceived in a more categorical way
## Accent attunement
* with exposure we begin to shift phoneme boundaries and attune to accent of the speaker
* process begins with as few as 2-4 sentences
* listening to just one speaker is not enough
* if you only hear speech from one speaker you will only attune to individual speaker not the accent as a whole
* if you hear speech from multiple speakers of the same dialect, will find it easier to understand novel speakers of the same dialect
* accent attunement can happen across accents
  * listening to 5 speakers with 5 diff accents can generalize to 6th nocel acccent
  * people who grown up with exposure to accented speech are more accurate at understanding speech in accents
  * willingness matters but will also eventually happen even if you don't want it to
## accomodation
* you start to talk the same way when you spend a lot of time with same group of ppl
* start to shift towards speech of the people around us
* basis of how dialects are formed
* how we feel about a person also affects accomodation
  * more likely to accomodate towards someone you like
  * if you don't like someone you may actually divereg
## Speech parsing
### Temporal perception
* not start by listening to each segment
* we hear speech as syllables and words immediately
  * listen as chunks
  * humans have troubke analyzing order of events shorter than 0,2 secs (length of a syllable)
  * evidence: hard to determine where extra sound (like uhm) comes in a sentence
* bottom up and top down information
  * bottom up: sounds we hear
  * top down: contextual information 
    * which word is likely in context
    * which word is more frequent
    * speaker normalization
* phonetic restoration
  * may not be able to hear all sounds in a speech stream in noisy setting
  * brain will try to fill in likely sounds based on top down information
  * we do not realize we aren't hearing anything
  * sometimes we hear things were' not even there
  * brains 'fill' in missing phonemes
  * why
    * language does not usually take place in quiet controlled settings
    * real world speech is messy
    * if only bottom up info used, often would be hard to discern speech
    * phonemic restoration is how we use redundancy to handle small speech errors
      * heuristic for error correction
### Non auditory information
* sight
* we use sight when we are processing speech
  * easier to understand someone when talking in person than over phone
  * **gesture**
    * types: creates meaning or physical result of producing speech
    * semantic gestures
      * smiling to show jokes
      * holding fingers close when talking about small things
    * physical results
      * rhyhmic head motion
        * head moves to match changes in pitch and loudness
        * when you speak loudly you move your head more
        * if someone is restraining your head you will naturally get quieter
          * pitch range will also drop
      * rhythmic face motion
        * physical movement of face and jaws
        * vowels in english can be distinguished by shape (rounding) and space between lips (height)
        * passive lip reading
        * sight aiding speech
* mcgurk
  * if visual and auditory info do not match up brain will be tricked into hearing something up
    * ba/ga (averaged out to da)
    * /b/ and /g/ are further apart acoustically articularity than either /b/ or /d/ or /g/ and /d/
    * visual information interfering with auditory information
* aerotactile information
  * sense of touch can affect what you hear
  * people listened to words that contained either aspirated or unaspirated stop
  * periodically applied inaudible puffs of air to diff areas of body
  * when there was puff of air, participants more likely to say they heard aspirated sound
  * aided speech perception for aspirated phoneme, but interfered with ability to identify unaspirated phoneme
  * did not have to be against neck including ankles
    * if you had hairy ankles effect was stronger
* slips of the ear
  * **mondegreens**
  * misheard sound lyrics
  * music lacks conversational context (top down informatiojn)
  * certain sounds are more subsceptible to slips of the ear
  * even bottom up info might be unreliable in songs
  * mondegreens can even be caused by something that comes a line later in a song (downstream mondegreen)
    * become entrenched on second or third time